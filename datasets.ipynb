{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_clients = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess_purchase100():\n",
    "    dataset_path = \"data/dataset_purchase\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        purchase_dataset = f.readlines()\n",
    "    x, y = [], []\n",
    "    for datapoint in purchase_dataset:\n",
    "        split = datapoint.rstrip().split(\",\")\n",
    "        label = int(split[0]) - 1  # The first value is the label\n",
    "        features = np.array(split[1:], dtype=np.float32)  # The next values are the features\n",
    "        x.append(features)\n",
    "        y.append(label)\n",
    "\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.int8)\n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.1, random_state=1234)\n",
    "\n",
    "    y_train_onehot = OneHotEncoder(sparse=False).fit_transform(np.expand_dims(y_train, axis=1))\n",
    "    y_test_onehot = OneHotEncoder(sparse=False).fit_transform(np.expand_dims(y_test, axis=1))\n",
    "\n",
    "    x_test = np.array(x_test,dtype=np.float32)\n",
    "    y_test_onehot = np.array(y_test_onehot,dtype=np.float32)\n",
    "\n",
    "    xs,ys = [[] for _ in range(num_clients)],[[] for _ in range(num_clients)]\n",
    "    for index in range(x_train.shape[0]):\n",
    "        xs[y_train[index]%num_clients].append(x_train[index])\n",
    "        ys[y_train[index]%num_clients].append(y_train_onehot[index])\n",
    "\n",
    "    train_datasets_list = []\n",
    "    for x_i,y_i in zip(xs,ys):\n",
    "        x_i = np.array(x_i, dtype=np.float32)\n",
    "        y_i = np.array(y_i, dtype=np.float32)\n",
    "        train_datasets_list.append([x_i,y_i,x_test,y_test_onehot])\n",
    "    \n",
    "    return train_datasets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets_list = preprocess_purchase100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(\"./datasets\"):\n",
    "    os.mkdir(\"./datasets\")\n",
    "if not os.path.exists(\"./datasets/train\"):\n",
    "    os.mkdir(\"./datasets/train\")\n",
    "    \n",
    "for i,dataset in enumerate(train_datasets_list):\n",
    "    if not os.path.exists(\"./datasets/train/%d\"%(i)):\n",
    "        os.mkdir(\"./datasets/train/%d\"%(i))\n",
    "    np.save(\"./datasets/train/%d/train_x.npy\"%(i),dataset[0])\n",
    "    np.save(\"./datasets/train/%d/train_y.npy\"%(i),dataset[1])\n",
    "    np.save(\"./datasets/train/%d/test_x.npy\"%(i),dataset[2])\n",
    "    np.save(\"./datasets/train/%d/test_y.npy\"%(i),dataset[3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4f6cee94ddd6f9fabdc4324b804092ca52476833d8ad2672a1fceec99749714"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dist': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
